

\section{Methods}

In our analysis, we use three different architectures and neural networks and
optimize their hyperparameters for the respective use case. Since Hornik
\textit{et al.} have shown that multi-layer perceptrons (\emph{MLP})
are universal
function approximators \cite{hornik1989}, we evaluate this architecture with
varying number of hidden nodes.

After that, we investigate the strength of
improvement of using a recurrent neural network. This type of neural network has
already been applied to time series prediction \cite{connor1994}, but suffers
from the vanishing gradient problem when capturing long-term dependencies in a
sequence. Because of that, we decide to evaluate only the Long Short-Term Memory
(\emph{LSTM})
network architecture \cite{hochreiter1997}, which was successfully applied in
various time series prediction tasks like anomaly detection \cite{malhotra2015},
stock price \cite{fischer2018} and protein disorder prediction
\cite{hanson2016}.

In the last years, convolutional neural networks (\emph{CNN}s) have improved
the results in image classification \cite{krizhevsky2012} and other computer
vision tasks. A \emph{CNN} learns features from the data in a hierarchical
way, for example combining pixels to edges, edges to more complex forms etc.
until a high-level classification can be done. The large success in computer
vision has inspired researchears in time series prediction to also apply
\emph{CNN}s \cite{cui2016, borovykh2017}, so we also evaluate this
architecture in our analysis. The same way as images are composed of
hierarchical features (e.g. a face consisting of eyes, that consist of
certain edges etc.) we assume that similar hierarchies of features can be found
in time series data.

The loss is measured using the classical mean squared error (\emph{MSE}) or 
the root mean squared error (\emph{RMSE}), because
LÃ³pez-Caraballo \textit{et al.} \cite{lopez2016} use this error function to
compare various other neural network approaches for Mackey-Glass time series
prediction. This function is computed in Equation~\ref{equ:rmse}.

\begin{equation}
    RMSE = \sqrt{\frac{\sum_{i=1}^n (y_i - \hat{y}_i)^2}{n}}
    \label{equ:rmse}
\end{equation}

In order to train our models, we apply the \emph{Adam} optimizer
\cite{kingma2014}. This famous optimization algorithm (more than 20.000
citations in less than 5 years) is commonly used to train neural networks and
improves the stochastic gradient descent algorithm. Other optimization
algorithms are not considered in order to keep the hyperparameter tuning
feasible.

We use two different types of noise. First, we consider $i.i.d.$ gaussian
noise which is added to the underlying function $f(t)$ at each point of time 
$t$,
as seen in Equation~\ref{equ:iid_noise}. The other type of noise is a
discretized Wiener process, as given in Equation~\ref{equ:wiener}.
This kind of stochastic process is used in various areas,
for example to study Brownian motion in Physics.
For more details about Wiener processes, refer to Schilling
\textit{et al} \cite{schilling2014}.

\begin{equation}
    \forall t: \quad x(t) = f(t) + y, \quad y \sim \mathcal{N}(0, \sigma^2)
    \label{equ:iid_noise}
\end{equation}

\begin{equation}
    \forall t: x(t) = f(t) + y(t), \enspace y(t) = y(t-1) + y, \enspace
        y \sim \mathcal{N}(0, \sigma^2), \enspace y(0) = 0
    \label{equ:wiener}
\end{equation}

All implementations are done using the \texttt{Python} programming language in
version $3.6.7$. The time series data is created and loaded in the
\texttt{numpy} framework in version $1.16.1$. In order to train the neural
networks both fast and elegant, we use the \texttt{keras} framework 
(version $2.2.4$) with the
\texttt{tensorflow} backend in version $1.13.1$.
The reason for this choice is the tight
integration between \texttt{keras} and \texttt{numpy} that simplifies and
increases the speed of our software development. Before running the experiments,
the random number generator of \texttt{numpy} is set to the seed $0$ to ensure
reproducibility of data generation.
The time series data for the biological oscillator
is generated using \texttt{gillespy} \cite{abel2016}, which only works with 
\texttt{Python} in version 2. For this application,
the version $2.7.15$ of \texttt{Python} is used.
