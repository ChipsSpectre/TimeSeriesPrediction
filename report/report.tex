\documentclass{article}

\title{Challenges in stochastic time series prediction}
\author{Maximilian Hornung, Jens Settelmeier}

\begin{document}
\maketitle

\begin{abstract}
    Predicting the future development of time series is of interest in different
    areas of computational biology. The time series in biological models exhibit
    challenging characteristics such as chaotic or stochastic behavior. In this
    report, time series prediction is done on different kind of biological time
    series using deep neural networks. In our evaluation, we identify challenges
    and limitations of this approach, and compare different architectures of
    deep neural networks with regard to their performance.
\end{abstract}

\section{Introduction}

In various kind of diseases, it is important to choose the correct treatment at
the current time point. Since the human body is too complex to be described
directly using mathematics, special features are modeled with mathematical
systems such as the Mackey-Glass equations for respiratory and hematopoietic 
diseases \cite{mackey1977}. These models make use of available data about
previous information in order to reduce the uncertainty under which a decision
is done. This is especially important when the consequences of a decision are
severe, for example the decision of breast cancer treatment after radiation.
This particular problem can be addressed by using a stochastic differential 
equation, as done by Oroji \textit{et al} \cite{oroji2016}.

In different kinds of biological models, chaotic behavior is observed. Even the
Mackey-Glass equations obtain chaotic behavior for certain choices of their
parameters, as shown by Fischer \cite{farmer1982}. That means that even small
perturbations of the time series prediction increase exponentially, which means
that the time series prediction should be as robust as possible against noise.

This gets imporant in particular because another characterstic of dealing with 
biological data is the presence of noise. In 
this report, we show that different kinds of noise assumptions can impact the
results of time series prediction. We evaluate all our investigated problems
with regard to the popular $i.i.d.$ random noise as well as memorizing random
noise. Since mathematical models of biological systems can not describe their
characteristics perfect and error-free, it is assumed that the latter type of
noise is more realistic. But unfortunately this kind of noise increases the
difficulty of time series prediction, even for simple mathematical functions, 
as seen in section~\ref{sec:sine}.

The remaining report is structured as follows. First, we perform proper time 
series prediction on the $sine$ function and show how different noise models
impact the prediction ability of our network models. We conduct an evaluation
how these results can be extended to the prediction of 
ordinary differential equations (\textbf{ODE}s) at the example of the 
differential equations of the harmonic oscillator. After that, we show how the
stochasticity of noise impactss the possibility to predict deterministic
chaotic time series at the example of the \emph{Mackey Glass} time series. Last
but not least, we report how the poor results of numerical approximations on 
stocastic differential equations (\textbf{SDE}s) can be explained based on our
previous results.

\section{Methods}

In our analysis, we use four different architectures and neural networks and
optimize their hyperparameters for the respective use case. Since Hornik 
\textit{et al.} have shown that feedforward neural networks are universal 
function approximators \cite{hornik1989}, we evaluate this architecture with 
varying number of hidden nodes. 

After that, we investigate the strength of
improvement of using a recurrent neural network. This type of neural network has
already been applied to time series prediction \cite{connor1994}, but suffers
from the vanishing gradient problem when capturing long-term dependencies in a 
sequence. Because of that, we decide to evaluate only the Long Short-Term Memory
(\textbf{LSTM})
network architecture \cite{hochreiter1997}, which was successfully applied in 
various time series prediction tasks like anomaly detection \cite{malhotra2015},
stock price \cite{fischer2018} and protein disorder prediction 
\cite{hanson2016}.

In the last years, convolutional neural networks (\textbf{CNN}s) have improved
the results in image classification \cite{krizhevsky2012} and other computer
vision tasks. A \textbf{CNN} learns features from the data in a hierarchical
way, for example combining pixels to edges, edges to more complex forms etc.
until a high-level classification can be done. The large success in computer
vision has inspired researchears in time series prediction to also apply
\textbf{CNN}s \cite{cui2016, borovykh2017}, so we also evaluate this 
architecture in our analysis.
% noise
% software

\section{Evaluation}
\subsection{Time series prediction of periodic functions}
\label{sec:sine}


\subsection{Time series prediction of ODEs}
% todo: advanced feature preparation
 
\subsection{Mackey Glass time series prediction}

In order to model diseases related to dynamic respiratory and hematopoietic 
diseases, Mackey \textit{et al.} proposed the mackey-glass equations, a kind of 
first-order nonlinear differential delay equations \cite{mackey1977}. If the 
delayed time ($x_{\tau} = x(t - \tau)$) exceeds the delay $\tau > 16.8$, then 
equation \ref{equ:mackey} behaves chaotic \cite{farmer1982}.

\begin{equation}
  \frac{dx}{dt} = \beta \cdot \frac{x_{\tau}}{1 + x_{\tau}^n}
  \label{equ:mackey}
\end{equation}

The first approach to predict the short-time behavior of chaotic time series
was done by Farmer \textit{et al.} who proposed a \texttt{local approximation}
technique \cite{farmer1987}. After improval of predictions using support vector
machines by MÃ¼ller \textit{et al.} \cite{muller1997}, the focus in research
shifted towards artifical neural networks which enable even better predictions.
Two of the latest developments are the usage of Wavelet Networks
\cite{alexandridis2013} and particle swarm optimization \cite{caraballo2016}.


\subsection{SDE time series prediction}
    
\section{Conclusion}

\bibliographystyle{plain}
\bibliography{report}
\end{document} 