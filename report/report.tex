\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{mathtools}
\usepackage{subcaption}

\title{Challenges in chaotic time series forecasting}
\author{Maximilian Hornung}

\begin{document}
\maketitle

\begin{abstract}
    Predicting the future development of time series is of interest in different
    areas of computational biology. The time series in biological systems exhibit
    challenging characteristics such as chaotic or stochastic behavior. In this
    report, time series prediction is done on several kind of biological time
    series using deep neural networks. We compare classical multi layer
    perceptrons with recurrent and convolutional architectures.
    In our evaluation, we identify challenges
    and limitations of this approach, and compare different architectures of
    deep neural networks with regard to their performance. We conclude that
    convolutional neural networks yield the lowest error. We also show how
    \textbf{Neural networks can be used to estimate how chaotic a time series is}
    and that a generation of stochastic time series is not possible using
    deterministic neural networks, because only the mean of the probability
    distribution can be estimated.
\end{abstract}

\section{Introduction}

In various kind of diseases, it is important to choose the correct treatment at
the current time point. Since the human body is too complex to be described
directly using mathematics, special features are modeled with mathematical
systems such as the Mackey-Glass equations for respiratory and hematopoietic
diseases \cite{mackey1977}. These models make use of available data about
previous information in order to reduce the uncertainty under which a decision
is done. This is especially important when the consequences of a decision are
severe, for example the decision of breast cancer treatment after radiation.
This particular problem can be addressed by using a stochastic differential
equation, as done by Oroji \textit{et al} \cite{oroji2016}.

In different kinds of biological models, chaotic behavior is observed. The
Mackey-Glass equations possess chaotic behavior for certain choices of their
parameters, as shown by Fischer \cite{farmer1982}. That means that even small
measurement errors of the initial state lead to exponentially increasing
errors of the time series prediction. Because of that, it is important
that the time series prediction should be as robust as possible against noise.

This gets imporant in particular because another characterstic of dealing with
biological data is the presence of noise. In
this report, we show that different kinds of noise assumptions can impact the
results of time series prediction. We evaluate all our investigated problems
with regard to the popular $i.i.d.$ random noise as well as noise from a
discretized Wiener process, i.e. memorizing random
noise. Since mathematical models of biological systems can not describe their
characteristics perfect and error-free, it is assumed that the latter type of
noise is more realistic. But unfortunately this kind of noise increases the
difficulty of time series prediction, even for simple mathematical functions,
as seen in Section~\ref{sec:sine}.

The remaining report is structured as follows. First, we perform proper time
series prediction on the sinus function and show how different noise models
impact the prediction ability of our network models. After that, we show how
different levels of chaos impact the possibility to forecast the development
of time series at the example of the \emph{Mackey Glass} time series. We show
how the stochasticity of noise impactss the possibility to predict deterministic
chaotic time series at the example of the \emph{Mackey Glass} time series. Last
but not least, we report fundamental limitations of neural networks to
approximate stochastic time series at the example of biological oscillators.

\section{Methods}

In our analysis, we use three different architectures and neural networks and
optimize their hyperparameters for the respective use case. Since Hornik
\textit{et al.} have shown that multi-layer perceptrons (\emph{MLP})
are universal
function approximators \cite{hornik1989}, we evaluate this architecture with
varying number of hidden nodes.

After that, we investigate the strength of
improvement of using a recurrent neural network. This type of neural network has
already been applied to time series prediction \cite{connor1994}, but suffers
from the vanishing gradient problem when capturing long-term dependencies in a
sequence. Because of that, we decide to evaluate only the Long Short-Term Memory
(\textbf{LSTM})
network architecture \cite{hochreiter1997}, which was successfully applied in
various time series prediction tasks like anomaly detection \cite{malhotra2015},
stock price \cite{fischer2018} and protein disorder prediction
\cite{hanson2016}.

In the last years, convolutional neural networks (\textbf{CNN}s) have improved
the results in image classification \cite{krizhevsky2012} and other computer
vision tasks. A \textbf{CNN} learns features from the data in a hierarchical
way, for example combining pixels to edges, edges to more complex forms etc.
until a high-level classification can be done. The large success in computer
vision has inspired researchears in time series prediction to also apply
\textbf{CNN}s \cite{cui2016, borovykh2017}, so we also evaluate this
architecture in our analysis. The same way as images are composed of
hierarchical features (e.g. a face consisting of eyes, that consist of
certain edges etc.) we assume that similar hierarchies of features can be found
in time series data.

The loss is measured using the root mean squared error (\emph{RMSE}), because
Caraballo \textit{et al.} \cite{caraballo2016} use this error function to
compare various other neural network approaches for Mackey Glass time series
prediction. This function is computed in Equation~\ref{equ:rmse}.

\begin{equation}
    RMSE = \sqrt{\frac{\sum_{i=1}^n (y_i - \hat{y}_i)^2}{n}}
    \label{equ:rmse}
\end{equation}

In order to train ours models, we apply the \emph{Adam} optimizer
\cite{kingma2014}. This famous optimization algorithm (more than 20.000
citations in less than 5 years) is commonly used to train neural networks and
improves the stochastic gradient descent algorithm. Other optimization
approaches are not considered to keep the grid searches feasible.

We use two different types of noise. First, we consider $i.i.d.$ gaussian
noise which is added to the underlying function $f(t)$
as seen in Equation~\ref{equ:iid_noise}. The other type of noise is a
discretized Wiener process, as given in Equation~\ref{equ:wiener}.
This type of processes is used in various areas,
for example to study Brownian motion in Physics.
For more details about Wiener processes, refer to Schilling
\textit{et al} \cite{schilling2014}.

\begin{equation}
    \forall t: \quad x(t) = f(t) + y, \quad y \sim \mathcal{N}(0, \sigma^2)
    \label{equ:iid_noise}
\end{equation}

\begin{equation}
    \forall t: x(t) = f(t) + y(t), \quad y(t) = y(t-1) + y \sim \mathcal{N}(0, \sigma^2), \quad y(0) = 0
    \label{equ:wiener}
\end{equation}

All implementation is done using the \texttt{Python} programming language in
version $3.6.7$. The time series data is created and loaded in the
\texttt{numpy} framework in version $1.16.1$. In order to train the neural
networks both fast and elegant, we use the \texttt{keras} (version $2.2.4$)
with the
\texttt{tensorflow} backend in version $1.13.1$.
The reason for this choice is the tight
integration between \texttt{keras} and \texttt{numpy} that simplifies and
increases the speed of our software development. Before running the experiments,
the random number generator of \texttt{numpy} is set to the seed $0$ to ensure
reproducibility of data generation.

\section{Evaluation}
\subsection{Time series forecasting of periodic functions}
\label{sec:sine}
In the first part of the analysis, we analyze the capability of different neural
network architectures to perform time series prediction on the periodic sinus
function. This task can be considered simple, because the different parts of the
sine wave occur multiple times in the data, i.e. the function is periodic.
It is therefore interesting how
robust the network architectures are with regard to different kinds of noise.

In this problem, we apply a large grid search over several parameters given in
Table~\ref{tab:gridparameters}. All models are trained using the \emph{Adam}
optimizer and $10$ time sequence entries are used to predict $1$ directly
consecutive point. These assumptions are made in order to keep the grid search
feasible. For training, testing and validation, a range from $0$ to $6 \pi$ is
used. Expressed more formally,
we provide information about the last 10 time sequence points
to the network and want to predict the next point, as given in
Equation~\ref{equ:sine}. That is, we want to estimate $x(t)$ with a function
$F(x(t), x(t-1), \cdots, x(t-9))$ that is computed using a neural network.

\begin{equation}
    \hat{x} (t + 1) \approx F(x(t), x(t-1), \cdots, x(t-9))
    \label{equ:sine}
\end{equation}


\begin{table}
    \centering
    \begin{tabular}{l|c}
        Parameter                   & Values                    \\
        \hline
        Number of hidden nodes      & [5, 10, 20]               \\
        std ($\sigma$) of noise     & [0, 0.01, 0.1]            \\
        Noise type                  & ["iid", "Wiener process"] \\
        Learning rate               & [0.001, 0.01, 0.1]        \\
        Epochs of training          & [10, 100, 500]            \\
        Neural network architecture & ["MLP", "LSTM", "CNN"]    \\
    \end{tabular}
    \caption{Parameter combinations used for the grid search. Each possible value
        of each parameter is combined with every possible value of each other
        parameter. Note that the \emph{CNN}s are evaluated with 1, 2, or 3 hidden
        layers of with 8 filters instead.}
    \label{tab:gridparameters}
\end{table}
%todo: mg adam justification

As we expected from our literature review, the \emph{LSTM} yielded lowest error
on the noise free time series data as seen in Table~\ref{tab:noisefree_result}.
After investigating the high error for the \emph{CNN} architecture, we discover
that this architecture needs a lower training rate than the \emph{LSTM}
architecture to yield good results -- the learning rate of $0.1$ is too high
and ruins the results as seen in Table~\ref{tab:cnn_training}.

\begin{table}
    \centering
    \begin{tabular}{l|c}
        Architecture & Average MSE \\
        \hline
        MLP          & 0.00285     \\
        LSTM         & 0.00066     \\
        CNN          & 0.07583     \\
    \end{tabular}
    \caption{Average MSE over all applied neural networks aggregated by
        the neural network architecture type.}
    \label{tab:noisefree_result}
\end{table}

\begin{table}
    \centering
    \begin{tabular}{l|c|c}
        Learning rate & Average MSE (CNN) & Average MSE (LSTM)       \\
        \hline
        0.1           & 0.227052          & 5.391024 $\cdot 10^{-5}$ \\
        0.01          & 0.000155          & 8.129514 $\cdot 10^{-5}$ \\
        0.001         & 0.000159          & 0.000276                 \\
    \end{tabular}
    \caption{Comparison of the average MSE when training \emph{CNN} and
        \emph{LSTM} architectures. We see that the \emph{LSTM} can be trained
        using higher learning rates.}
    \label{tab:cnn_training}
\end{table}

Next we investigate based on our grid search the influence of the type of noise
and the standard deviation of the noise on the results. The average error is
strongly impacted as seen in Table~\ref{tab:wiener_iid}. We see that
a higher $\sigma$ increases the test error, as expected. But it seems that the 
neural networks are less capable of filtering noise from a Wiener process, and 
for a high $\sigma$ of this type of noise completely lose the ability to 
approximate the underlying sinus function on the first look. A further analysis
of the used networks architectures showed that this is merely by inluding 
\emph{CNN}s trained with learning rate $0.1$. After excluding them, the test 
error goes down to a high but reasonable result.

\begin{table}
    \centering
    \begin{tabular}{l|ccc}
        Type of noise & MSE & MSE ($\sigma=0.01$) & MSE($\sigma=0.1)$) \\
        \hline
        $i.i.d$ noise &   0.015 & 0.008  &  0.022 \\
        Wiener process & 24.153 & 0.0524 & 48.254 (1.322) \\
    \end{tabular}
    \caption{Average MSE aggregated by the type of noise and the noise
        standard deviation.}
    \label{tab:wiener_iid}
\end{table}

In Figure~\ref{fig:sineresult}, we see the best neural networks found using 
the large grid search for a noise free time series. The approximation is 
remarkably accurate and no deviation can be found. If we add different kinds of 
noise, the grid search is still able to find suitable architectures, 
as seen in Figure~\ref{fig:sineresult}. In Figure~\ref{fig:histogram}
we demonstrate that only a few outliers caused the average error 
for Wiener process noise with high $\sigma$ to increase so strongly.

\begin{figure}
    \centering
    \includegraphics[width=0.6\textwidth]{figures/Noise_free.pdf}
    \caption{Demonstration of grid search results. For the noise free case,
    a nearly perfect approximation is achieved.}
    \label{fig:sinenoisefree}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.45\textwidth]{figures/iid_low_noise.pdf}
    \includegraphics[width=0.45\textwidth]{figures/iid_high_noise.pdf}
    \includegraphics[width=0.45\textwidth]{figures/wiener_low_noise.pdf}
    \includegraphics[width=0.45\textwidth]{figures/wiener_high_noise.pdf}
    \caption{Demonstration of grid search results. The low noise has a
    standard deviation of $\sigma =0.01$ and the high noise uses $\sigma=0.1$,
    independent of the type of noise.}
    \label{fig:sineresult}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=0.6\textwidth]{figures/histogram.pdf}
    \caption{Demonstration of grid search results. A few networks with high
    test error have to be considered during the evaluation to get more 
    robust interpretations.}
    \label{fig:histogram}
\end{figure}

In Figure~\ref{fig:noise_impact}, we see that a \emph{MLP}
with one hidden layer of 10 nodes is capable of performing the time series
prediction. Even if we apply $i.i.d.$ gaussian noise with standard
deviation $\sigma = 0.1$, the
network can still be trained to fit the data correctly. But if the noise is not
applied independently in each timestep, the convergence takes more time steps as
pointed out in Figure~\ref{fig:noise_loss_impact}. This effect is especially
strong for the \emph{LSTM} architecture, where even 100 epochs are not enough
for the validation loss to converge, compared to the other approaches that need
only 20 epochs.

Since the noise we apply on the data has a standard deviation $\sigma = 0.1$,
it is obvious that a perfect time series prediction is not be possible. We
expect that the minimum possible validation loss is as high as the variance
($\sigma^2$), and can reproduce that in all neural network architectures as
shown in Table~\ref{tab:noise_finals}. The validation loss of a perfect
neural network would therefore be $\sigma^2 = 0.01$, which is achieved by all
architectures for $i.i.d.$ noise. Only the LSTM architecture has worse
performance for the memorizing noise. We assume that this is due to the slower
learning of that architecture compared to the others when confronted with
memorizing noise. One possible reason for this observation is that the LSTM is
trying to capture temporal dependencies between points in the time series, which
is confused by the temporal dependencies induced by the memorizing noise.

\begin{table}
    \centering
    \begin{tabular}{l|c|c|c}
        Architecture & Noisefree data & $i.i.d.$ noise & memorizing noise \\
        \hline
        MLP          & 0.0021         & 0.0249         & 0.0174           \\
        LSTM         & 0.0021         & 0.0127         & 0.5554           \\
        CNN          & 0.0021         & 0.0136         & 0.0234           \\
    \end{tabular}
    \caption{Validation loss of different neural network architectures for
        time series prediction on the sinus function stated
        in RMSE.}
    \label{tab:noise_finals}
\end{table}

\subsection{Mackey Glass time series forecasting}

In order to model diseases related to dynamic respiratory and hematopoietic
diseases, Mackey \textit{et al.} proposed the mackey-glass equations, a kind of
first-order nonlinear differential delay equations to model
the number of white blood cells over time \cite{mackey1977}. The solutions to
these equations given in Equation~\ref{equ:mackey}
exhibit chaotic behavior under certain conditions \cite{farmer1982}. For fixed
parameters $a = 0.2$, $b=0.1$ and $c=10$ this system has a stable fixed point
attractor for $\tau < 4.53$. With increasing delay time, the system gets less
stable. For delay times of $4.53 < \tau < 13.3$ there is a limit cycle attractor
whose period raises for $13.3 \leq \tau \leq 16.8$. For delay time
$\tau > 16.8$ the system shows chaotic behavior.

\begin{equation}
    \frac{dx}{dt} = \frac{a \cdot x(t - \tau)}{1 + x(t - \tau)^c} - b \cdot x(t)
    \label{equ:mackey}
\end{equation}

By using the Euler method to discretize the Mackey Glass time series, we can
see that the chaotic behavior gets stronger for increasing $\tau > 16.8$. In
Figure~\ref{fig:mackey_chaos}, we simulate the solution of the Mackey Glass
time series for slightly different initial conditions. It is clearly visible
that for $\tau = 17$, the time series diverges slower than for $\tau = 25$.

The first approach to predict the short-time behavior of chaotic time series
was done by Farmer \textit{et al.} who proposed a \texttt{local approximation}
technique \cite{farmer1987}. After improval of predictions using support vector
machines by Müller \textit{et al.} \cite{muller1997}, the focus in research
shifted towards artifical neural networks which enable even better predictions.
Two of the latest developments are the usage of Wavelet Networks
\cite{alexandridis2013} and particle swarm optimization \cite{caraballo2016}.

\begin{equation}
    x(t+1) = x(t) + \frac{\beta x(t - \tau)}{1 + x^{n}(t - \tau)} - \gamma x(t)
    \label{equ:mackey_euler}
\end{equation}

\begin{figure}
    \centering
    \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/mg_chaos_17.pdf}
    \end{subfigure}
    \hspace{-6mm}
    \begin{subfigure}{.5\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/mg_chaos_25.pdf}
    \end{subfigure}
    \caption{Influence of time delay parameter $\tau$ on the chaotic behavior of
        the Mackey Glass equation. The left side of the figure uses $\tau = 17$,
        the right side $\tau = 25$.}
    \label{fig:mackey_chaos}
\end{figure}

In accordance with the approach used by Caraballo \textit{et al.}
\cite{caraballo2016}, we predict $x(t+6)$ based on the information of the time
series points in $x(t)$, $x(t-6)$, $x(t-12)$, and $x(t-18)$. We investigate how
an increase in chaotic behavior impacts the prediction capability of the
different neural network architectures. The results for the feedforward neural
network are depicted in Figure~\ref{fig:mackey_cnn}. For this plot, a 2-layer
LSTM architecture was used, using 10 hidden nodes in the LSTM layer,
followed by a feedforward layer to sum up for the output value.
The first 500 time points of the time series are used for training, the 500
following points for validation. Additional 500 points are not used during the
training procedure at all and form the test set. For training our models, we
use a learning rate of $0.001$ for the Adam optimizer, which is the default
value suggested by the software framework and could be shown to be suitable for
time series prediction in section \ref{sec:sine}.

It can be seen that for non-chaotic solutions of the macke-glass time series the
model is able to predict precisely, and that for values of $\tau > 20$ the
chaotic behavior is strong enough to impact the prediction. We explain the low
error for $\tau = 21$ in the ability of the network to compensate for the amount
of chaos up to this point.

\begin{figure}
    \centering
    \includegraphics[width=0.6\textwidth]{figures/mackey_glass_cnn.pdf}
    \caption{Validation error of the convolutional neural network for different
        time delay values $\tau$ of the Mackey Glass equation. The error increases
        strongly for values of $\tau > 20$, indicating a strong chaotic behavior
        from this point.}
    \label{fig:mackey_cnn}
\end{figure}

In the next step, we compare our feedforward neural network and our LSTM
architecture with the results stated by Caraballo \textit{et al.}
\cite{caraballo2016} in Table~\ref{tab:mackey_results}. We
compare our results with a linear model, a cascade correlation neural network
and a radial basis function (\emph{RBF})
neural network additional to their approach to
combine an artificial neural network (\emph{ANN}) with particle swarm
optimization (\emph{PSO}) By
using the \emph{ReLU} (Rectified Linear Unit) activation function and two
hidden layers with 32 and 16 nodes, respectively, we outperform the
backpropagation network reported by Caraballo \textit{et al}.
From our considered models, the \emph{CNN} achieves the lowest RMSE value and
allows for accurate prediction, both for validation and for test data as seen
in Figure~\ref{fig:mackey_pred}. This \emph{CNN} uses three convolutional layers
with 1D convolutions, kernel size of 3, "same" padding and 8
filters each. A classical
feedforward layer without activation function is used to extract the output
value for the time series prediction.

The \emph{LSTM} employs 10 LSTM nodes, followed by a feedforward layer to sum up
the results. A variety of other layer configurations has been applied
on the problem,
but all consistently perform worse than the other neural network architectures.
We assume that the chaotic behavior of the time series confuses the \emph{LSTM}
which is trying to account for temporal dependencies between the time series
points.

\begin{figure}
    \centering
    \includegraphics[width=\linewidth]{figures/mg_pred_cnn.pdf}
    \caption{The prediction of the noisefree Mackey Glass time series using a
        \emph{CNN}. The precise predictions for training, validation and testing
        points show the accurate approximation of the chaotic time series.}
    \label{fig:mackey_pred}
\end{figure}

\begin{table}
    \centering
    \begin{tabular}{c|c}
        Method                         & $RMSE_{x(t+6)}$                          \\
        \hline
        Linear model                   & 0.5503                                   \\
        Cascade correclation NN        & 0.0624                                   \\
        \textit{proposed LSTM}         & 0.0131                                   \\
        \textit{proposed MLP}          & 0.0125 (vs. 0.0262 \cite{caraballo2016}) \\
        RBFNN                          & 0.0114                                   \\
        \textit{proposed CNN}          & 0.0075                                   \\
        ANN + PSO \cite{caraballo2016} & 0.0053                                   \\
    \end{tabular}
    \caption{Mackey Glass time series forecasting results using different neural
        network architectures.}
    \label{tab:mackey_results}
\end{table}

Now we apply noise on the mackey glass time series to account for the imperfect
data characteristic to biological measurements. The results for these
experiments are stated in Table~\ref{tab:mackey_noise}. We see that an increased
standard deviation of the added noise increases the test error, measured in
RMSE. This applies to all neural network architectures.

We interpret the results of Table~\ref{tab:mackey_noise}
as the neural networks are capable of dealing with a certain
amount of noise. For example the \emph{CNN} has
approximately 9 times higher error rate when increasing the $\sigma$ of the
Wiener process from $0.01$ to $0.1$.

But more interestingly,
the negative effect of the noise is stronger if the noise is memorizing, i.e. if
the effect of the noise in time step $t$ impacts the time series in time step
$t + 1$. We explain this with an increase in chaotic behavior induced by this
type of noise to the Mackey Glass time series. Since chaotic time series by
definition deviate exponentially based on small variations in the initial
conditions, it seems logical that this divergence is increased by introducing
perturbations of the time series in every time step.

\begin{table}
    \centering
    \begin{tabular}{c|c|c|c}
        Architecture & noise type & $\sigma$ & RMSE   \\
        \hline
        CNN          & iid        & 0.01     & 0.0196 \\
        CNN          & iid        & 0.1      & 0.0806 \\
        CNN          & wiener     & 0.01     & 0.0242 \\
        CNN          & wiener     & 0.1      & 0.2172 \\
        MLP          & iid        & 0.01     & 0.0249 \\
        MLP          & iid        & 0.1      & 0.0949 \\
        MLP          & wiener     & 0.01     & 0.0348 \\
        MLP          & wiener     & 0.1      & 0.2162 \\
        LSTM         & iid        & 0.01     & 0.0432 \\
        LSTM         & iid        & 0.1      & 0.0862 \\
        LSTM         & wiener     & 0.01     & 0.0347 \\
        LSTM         & wiener     & 0.1      & 0.2044 \\
    \end{tabular}
    \caption{Results of the Mackey Glass time series forecasting using different
        types of neural networks, noise levels and types.}
    \label{tab:mackey_noise}
\end{table}

\subsection{Biological oscillator time series forecasting}

The next section deals with time series forecasting of biological oscillators,
as described by Novak \textit{et al} \cite{novak2008}. For different aspects of
cell physiology, e.g. the DNA synthesis, this kind of oscillators can be
observed. Novak \textit{et al.} show that oscillations in simple ODEs can be
produced by incorporating an explicit time delay \cite{novak2008}, similar to
the Mackey Glass equations \cite{mackey1977}.

Recently, Strömstedt \textit{et al.} analyzed the capability of neural network
architectures to model stochastic time series \cite{stroemstedt2018} at the
example of time series produced according to the characteristics of Novak
\textit{et al} \cite{novak2008}. In this section, we reproduce their results for
the LSTM architecture and state fundamental limits for neural networks to
approximate stochastic time series.

The time series data is generated using \texttt{gillespy} \cite{abel2016}.
Using this framework, a simple stochastic reaction system with 2 states is
set up in Equation~\ref{equ:oscillator}.
One protein $X$ is produced more the less of the other protein $Y$
exists. $X$ and $Y$ are both continuously reduced but $Y$ gets reduced stronger
for smaller values of $Y$. A set of 6 parameters
($k_{dx}, k_t, k_{dy}, a_0, a_1, a_2)$
is used to parametrize the system.

\begin{align}
    \frac{dX}{dt} = & \frac{1}{1+Y^2} - k_{dx} \cdot X                                           \\
    \frac{dY}{dt} = & k_t \cdot X - k_{dy} \cdot Y - \frac{Y}{a_0 + a_1 \cdot Y + a_2 \cdot Y^2}
    \label{equ:oscillator}
\end{align}

The motivation for using neural netowrks to generate time series is the
computational expensiveness of analytical solutions like the Gillespie
algorithm \cite{gillespie1977}. The runtime of an experiment on our hardware
(see Table~\ref{tab:hardware} for details)
takes more than 6 seconds runtime to simulate the biological oscillator with two
states for 100 time steps (yielding only one trajectory of the stochastic
system). This makes grid search approaches on the large number
of parameter
combinations intractable. Using the 2-layer LSTM approach, predictions for
short time series can be done in less than $0.2$ seconds on our hardware.

The goal here is to train a LSTM based
model to convert a set of input parameters into a time series. The first
LSTM layer
takes 7 input values and transforms them into $n$ sequences of 7 values, with
$n$ being the desired length of the output sequence. The second LSTM layer
takes this result and converts it to a sequence of length $n$.

The analysis of the resulting time series as depicted in
Figure~\ref{fig:nn_limitation} shows the general limitation of using a
deterministic neural network for predicting a stochastic time series. For one
possible of parameters for the biological oscillator, the analytical solution
was computed under $100$ (stochastic) trajectories. After that, the network was
trained to predict the time series based on the (unchanged) parameters. We see
that the short-term predictions are accurate and have the same magnitude as the
ground truth signal -- but the more time steps are simulated, the larger is the
difference between the ground truth trajectories due to stochasticity. In other
words, the neural network predicts the mean of the signals and the amplitude
decreases due to the increasing variance of the signal over time.
It should be noted that the trends in the time series are still
recognized, just with a smaller amplitude.

\begin{table}
    \centering
    \begin{tabular}{cc}
        Processor & Intel(R) Core i5-7200U CPU @ 2.50GHz \\
        RAM       & 7859 MB                              \\
    \end{tabular}
    \caption{Hardware configuration used in experiments.}
    \label{tab:hardware}
\end{table}

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/nn_limitation.pdf}
    \caption{Prediction of stochastic time series using deterministic
        \textbf{LSTM} network. Sampled ground truth data are unseen for the network.
        Mean computed on all stochastic trajectories.}
    \label{fig:nn_limitation}
\end{figure}

\section{Conclusion}

In this report, we investigate how chaotic time series can be approximated
using neural networks. It is possible to improve the forecasting results by
advanced architectures. Especially the convolutional neural networks
outperform the feedforward neural network with one hidden layer,
although theoretically all classes are capable of approximating any continuous
function. We assumed that the hierarchical features in time series can be
captured better using a \emph{LSTM} by capturing time dependencies or a
\emph{CNN} for extracting hierarchical features, but the results indicate that
the convolutional neural networks also show superior performance in this area.
Unfortunately, it is also shown that neural networks can only be used to
generate stochastic time series in a limited way. Since the prediction operation
is deterministic, the possibility of predicting the stochastic time series
declines with the length of the prediction. Only the general trends can be seen
from the neural network, the amplitude declines below the level of the
original time series.

\bibliographystyle{alpha}
\bibliography{report}

\end{document}